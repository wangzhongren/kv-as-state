# -*- coding: utf-8 -*-

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

MODEL_DIR = "./Qwen/Qwen3-0.6B"
DTYPE = torch.float16

MAX_NEW_TOKENS = 1024
TEMPERATURE = 0.7

print("ğŸ” åŠ è½½æ¨¡å‹...")

tokenizer = AutoTokenizer.from_pretrained(
    MODEL_DIR,
    trust_remote_code=True,
    local_files_only=True
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_DIR,
    torch_dtype=DTYPE,
    device_map="auto",
    trust_remote_code=True,
    local_files_only=True
)

model.eval()
print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")

# ======================
# ä¿å­˜å’ŒåŠ è½½ KV çŠ¶æ€çš„å‡½æ•°
# ======================

def save_past_kv(past_kv, filename='past_kv.pth'):
    if past_kv is not None:
        torch.save(past_kv, filename)
        print(f"âœ… KV çŠ¶æ€å·²ä¿å­˜åˆ° {filename}")

def load_past_kv(filename='past_kv.pth'):
    if os.path.exists(filename):
        past_kv = torch.load(filename)
        # å°†æ‰€æœ‰å¼ é‡ç§»åŠ¨åˆ°æ¨¡å‹è®¾å¤‡ï¼ˆé€’å½’å¤„ç†å…ƒç»„/åˆ—è¡¨ç»“æ„ï¼‰
        def move_to_device(item):
            if isinstance(item, torch.Tensor):
                return item.to(model.device)
            elif isinstance(item, tuple):
                return tuple(move_to_device(i) for i in item)
            elif isinstance(item, list):
                return [move_to_device(i) for i in item]
            return item
        past_kv = move_to_device(past_kv)
        print(f"âœ… KV çŠ¶æ€å·²ä» {filename} åŠ è½½")
        return past_kv
    else:
        print(f"âš ï¸ æ–‡ä»¶ {filename} ä¸å­˜åœ¨ï¼Œä½¿ç”¨ None ä½œä¸ºåˆå§‹ KV")
        return None

# ======================
# è¿ç»­ KV çŠ¶æ€ç”Ÿæˆå‡½æ•°
# ======================

@torch.no_grad()
def generate_continuous(prompt, past_kv=None):
    input_text = build_chat_prompt(prompt)

    input_ids = tokenizer(
        input_text,
        return_tensors="pt",
        add_special_tokens=False
    ).input_ids.to(model.device)

    outputs = model(
        input_ids=input_ids,
        past_key_values=past_kv,
        use_cache=True
    )

    past_kv = outputs.past_key_values
    logits = outputs.logits[:, -1, :] / TEMPERATURE

    generated = input_ids.clone()

    for _ in range(MAX_NEW_TOKENS):
        probs = torch.softmax(logits, dim=-1)
        next_id = torch.multinomial(probs, 1)

        generated = torch.cat([generated, next_id], dim=1)

        outputs = model(
            input_ids=next_id,
            past_key_values=past_kv,
            use_cache=True
        )

        past_kv = outputs.past_key_values
        logits = outputs.logits[:, -1, :] / TEMPERATURE

        if next_id.item() == tokenizer.eos_token_id:
            break

    text = tokenizer.decode(
        generated[0][input_ids.shape[1]:],
        skip_special_tokens=True
    )

    return text.strip(), past_kv

# ======================
# æ„å»ºèŠå¤©æç¤ºå‡½æ•°
# ======================

def build_chat_prompt(user_text):
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªç®€æ´ã€å‡†ç¡®çš„ä¸­æ–‡åŠ©æ‰‹ã€‚"},
        {"role": "user", "content": user_text}
    ]
    return tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

# ======================
# æµ‹è¯•ï¼ˆæ”¯æŒä¿å­˜å’Œé‡æ”¾ï¼‰
# ======================

# å…ˆåŠ è½½å·²ä¿å­˜çš„ KVï¼ˆå¦‚æœå­˜åœ¨ï¼‰
past_kv = load_past_kv('past_kv.pth')  # å¯ä»¥è‡ªå®šä¹‰æ–‡ä»¶å

dialogs = [
    "å·´é»æ˜¯å“ªä¸ªå›½å®¶çš„é¦–éƒ½ï¼Ÿ",
    "å®ƒæœ‰å“ªäº›è‘—åæ™¯ç‚¹ï¼Ÿ",
    "è¯·ç”¨ä¸€å¥è¯èµç¾å®ƒã€‚"
]

# dialogs = [
#     "å®ƒçš„äººå£å¤§çº¦æœ‰å¤šå°‘ï¼Ÿ",
#     "ä¸ºä»€ä¹ˆè¯´å®ƒæ˜¯æµªæ¼«ä¹‹éƒ½ï¼Ÿ",
#     "ç»™æˆ‘æ¨èä¸€ä¸ªå·´é»çš„ä¸‰å¤©æ—…è¡Œè¡Œç¨‹ã€‚"
# ]

for i, q in enumerate(dialogs, 1):
    print(f"\n=== å¯¹è¯ {i} ===")
    print(q)
    ans, past_kv = generate_continuous(q, past_kv)
    print(ans)
    # å¯é€‰ï¼šåœ¨æ¯ä¸ªå¯¹è¯åä¿å­˜ KV
    # save_past_kv(past_kv, f'past_kv_turn{i}.pth')

# æ‰€æœ‰å¯¹è¯ç»“æŸåä¿å­˜ KVï¼ˆç”¨äºä¸‹æ¬¡é‡æ”¾ï¼‰
save_past_kv(past_kv, 'past_kv.pth')